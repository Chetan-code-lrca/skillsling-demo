import streamlit as st
import ollama
import time
import tempfile
import os
import json
from datetime import datetime
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

# ==================== CONFIG ====================
AVAILABLE_MODELS = ["llama3.2:3b", "gemma2:2b", "phi3:mini"]

PLACEHOLDERS = {
    "Hindi": "‡§Ö‡§™‡§®‡§æ ‡§∏‡§µ‡§æ‡§≤ ‡§Ø‡§π‡§æ‡§Å ‡§≤‡§ø‡§ñ‡•á‡§Ç...",
    "English": "Type your question here...",
    "Hinglish": "Apna sawaal yahan likhein...",
    "Tamil": "‡Æâ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æï‡Øá‡Æ≥‡Øç‡Æµ‡Æø‡ÆØ‡Øà ‡Æá‡Æô‡Øç‡Æï‡Øá ‡Æé‡Æ¥‡ØÅ‡Æ§‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç...",
    "Telugu": "‡∞Æ‡±Ä ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞®‡±Å ‡∞á‡∞ï‡±ç‡∞ï‡∞° ‡∞ü‡±à‡∞™‡±ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø..."
}

# ULTRA-STRONG LANGUAGE ENFORCEMENT ‚Äì Repeat + examples + negative reinforcement
LANGUAGE_SYSTEM_PROMPTS = {
    "Hindi": """‡§§‡•Å‡§Æ ‡§è‡§ï ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§∂‡§ø‡§ï‡•ç‡§∑‡§ï ‡§π‡•ã‡•§
‡§Ö‡§®‡§ø‡§µ‡§æ‡§∞‡•ç‡§Ø ‡§®‡§ø‡§Ø‡§Æ (3 ‡§¨‡§æ‡§∞ ‡§¶‡•ã‡§π‡§∞‡§æ‡§ì):
1. ‡§ï‡•á‡§µ‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§¶‡•á‡§µ‡§®‡§æ‡§ó‡§∞‡•Ä ‡§≤‡§ø‡§™‡§ø ‡§Æ‡•á‡§Ç ‡§â‡§§‡•ç‡§§‡§∞ ‡§¶‡•ã‡•§ ‡§ï‡•á‡§µ‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç‡•§ ‡§ï‡•á‡§µ‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç‡•§
2. ‡§ï‡•ã‡§à ‡§≠‡•Ä ‡§Ö‡§Ç‡§ó‡•ç‡§∞‡•á‡§ú‡•Ä ‡§Ö‡§ï‡•ç‡§∑‡§∞, ‡§∂‡§¨‡•ç‡§¶, ‡§µ‡§æ‡§ï‡•ç‡§Ø ‡§Æ‡§§ ‡§≤‡§ø‡§ñ‡•ã ‚Äì ‡§è‡§ï ‡§≠‡•Ä ‡§®‡§π‡•Ä‡§Ç‡•§
3. ‡§§‡§ï‡§®‡•Ä‡§ï‡•Ä ‡§∂‡§¨‡•ç‡§¶‡•ã‡§Ç ‡§ï‡•ã ‡§≠‡•Ä ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§Æ‡•á‡§Ç ‡§∏‡§Æ‡§ù‡§æ‡§ì‡•§
4. ‡§∏‡§∞‡§≤ ‡§π‡§ø‡§Ç‡§¶‡•Ä ‡§¨‡•ã‡§≤‡•ã‡•§
5. ‡§õ‡§æ‡§§‡•ç‡§∞ ‡§ï‡•ã ‡§™‡•ç‡§∞‡•ã‡§§‡•ç‡§∏‡§æ‡§π‡§ø‡§§ ‡§ï‡§∞‡•ã‡•§
‡§ó‡§≤‡§§ ‡§â‡§¶‡§æ‡§π‡§∞‡§£: "Photosynthesis is..."
‡§∏‡§π‡•Ä ‡§â‡§¶‡§æ‡§π‡§∞‡§£: "‡§™‡•ç‡§∞‡§ï‡§æ‡§∂ ‡§∏‡§Ç‡§∂‡•ç‡§≤‡•á‡§∑‡§£ ‡§è‡§ï ‡§™‡•ç‡§∞‡§ï‡•ç‡§∞‡§ø‡§Ø‡§æ ‡§π‡•à ‡§ú‡§ø‡§∏‡§Æ‡•á‡§Ç ‡§™‡•å‡§ß‡•á ‡§∏‡•Ç‡§∞‡•ç‡§Ø ‡§ï‡•Ä ‡§∞‡•ã‡§∂‡§®‡•Ä ‡§∏‡•á ‡§≠‡•ã‡§ú‡§® ‡§¨‡§®‡§æ‡§§‡•á ‡§π‡•à‡§Ç‡•§"
NCERT ‡§¶‡§ø‡§∂‡§æ-‡§®‡§ø‡§∞‡•ç‡§¶‡•á‡§∂‡•ã‡§Ç ‡§ï‡§æ ‡§™‡§æ‡§≤‡§® ‡§ï‡§∞‡•ã‡•§""",
    
    "English": """You are an English tutor for Indian students.
MANDATORY RULES (repeat 3 times):
1. Respond ONLY in English language. ONLY in English. ONLY in English.
2. Use simple, clear English words.
3. Explain concepts in an easy-to-understand manner.
4. Encourage the student positively.
5. Follow NCERT curriculum guidelines.
Wrong example: "‡§´‡•ã‡§ü‡•ã‡§∏‡§ø‡§Ç‡§•‡•á‡§∏‡§ø‡§∏ ‡§ï‡•ç‡§Ø‡§æ ‡§π‡•à?"
Correct example: "Photosynthesis is the process by which plants make their own food using sunlight..."
Be detailed if the question requires explanation.""",
    
    "Hinglish": """Tum ek Hinglish tutor ho (Roman script mein Hindi + English mix).
MANDATORY RULES (repeat 3 times):
1. Hindi aur English dono ko mix karke likhna hai. ONLY Roman script.
2. Devanagari (‡§Ö, ‡§Ü) mat use karo.
3. Simple words use karo jo students samajh sakein.
4. Student ko encourage karo.
Example:
Question: "Photosynthesis kya hai?"
Response: "Photosynthesis ek process hai jisme plants apna khana banate hain sunlight use karke..."
NCERT guidelines follow karo.""",
    
    "Tamil": """‡Æ®‡ØÄ‡Æô‡Øç‡Æï‡Æ≥‡Øç ‡Æí‡Æ∞‡ØÅ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡ÆÜ‡Æö‡Æø‡Æ∞‡Æø‡ÆØ‡Æ∞‡Øç.
‡Æï‡Æü‡Øç‡Æü‡Ææ‡ÆØ ‡Æµ‡Æø‡Æ§‡Æø‡Æï‡Æ≥‡Øç (3 ‡ÆÆ‡ØÅ‡Æ±‡Øà ‡Æö‡Øä‡Æ≤‡Øç‡Æ≤‡ØÅ‡Æô‡Øç‡Æï‡Æ≥‡Øç):
1. ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øá ‡Æ™‡Æ§‡Æø‡Æ≤‡Æ≥‡Æø‡Æï‡Øç‡Æï ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç. ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç. ‡ÆÆ‡Æü‡Øç‡Æü‡ØÅ‡ÆÆ‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç.
2. ‡ÆÜ‡Æô‡Øç‡Æï‡Æø‡Æ≤ ‡Æé‡Æ¥‡ØÅ‡Æ§‡Øç‡Æ§‡ØÅ‡Æï‡Øç‡Æï‡Æ≥‡Øç ‡ÆÖ‡Æ≤‡Øç‡Æ≤‡Æ§‡ØÅ ‡Æµ‡Ææ‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡Øà‡Æï‡Æ≥‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§ ‡Æµ‡Øá‡Æ£‡Øç‡Æü‡Ææ‡ÆÆ‡Øç ‚Äì ‡Æí‡Æ©‡Øç‡Æ±‡ØÅ ‡Æï‡ØÇ‡Æü ‡Æá‡Æ≤‡Øç‡Æ≤‡Øà.
3. ‡Æ§‡Øä‡Æ¥‡Æø‡Æ≤‡Øç‡Æ®‡ØÅ‡Æü‡Øç‡Æ™ ‡Æö‡Øä‡Æ±‡Øç‡Æï‡Æ≥‡Øà‡ÆØ‡ØÅ‡ÆÆ‡Øç ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Æø‡Æ≤‡Øç ‡Æµ‡Æø‡Æ≥‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç.
4. ‡Æé‡Æ≥‡Æø‡ÆØ ‡Æ§‡ÆÆ‡Æø‡Æ¥‡Øç ‡Æö‡Øä‡Æ±‡Øç‡Æï‡Æ≥‡Øà‡Æ™‡Øç ‡Æ™‡ÆØ‡Æ©‡Øç‡Æ™‡Æü‡ØÅ‡Æ§‡Øç‡Æ§‡Æµ‡ØÅ‡ÆÆ‡Øç.
5. ‡ÆÆ‡Ææ‡Æ£‡Æµ‡Æ∞‡Øà ‡Æä‡Æï‡Øç‡Æï‡ØÅ‡Æµ‡Æø‡Æï‡Øç‡Æï‡Æµ‡ØÅ‡ÆÆ‡Øç.
‡Æ§‡Æµ‡Æ±‡Ææ‡Æ© ‡Æâ‡Æ§‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Øç: "Photosynthesis is..."
‡Æö‡Æ∞‡Æø‡ÆØ‡Ææ‡Æ© ‡Æâ‡Æ§‡Ææ‡Æ∞‡Æ£‡ÆÆ‡Øç: "‡Æí‡Æ≥‡Æø‡Æö‡Øç‡Æö‡Øá‡Æ∞‡Øç‡Æï‡Øç‡Æï‡Øà ‡Æé‡Æ©‡Øç‡Æ™‡Æ§‡ØÅ..."
NCERT ‡Æµ‡Æ¥‡Æø‡Æï‡Ææ‡Æü‡Øç‡Æü‡ØÅ‡Æ§‡Æ≤‡Øç‡Æï‡Æ≥‡Øà‡Æ™‡Øç ‡Æ™‡Æø‡Æ©‡Øç‡Æ™‡Æ±‡Øç‡Æ±‡Æµ‡ØÅ‡ÆÆ‡Øç.""",
    
    "Telugu": """‡∞Æ‡±Ä‡∞∞‡±Å ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞â‡∞™‡∞æ‡∞ß‡±ç‡∞Ø‡∞æ‡∞Ø‡±Å‡∞≤‡±Å.
‡∞§‡∞™‡±ç‡∞™‡∞®‡∞ø‡∞∏‡∞∞‡∞ø ‡∞®‡∞ø‡∞Ø‡∞Æ‡∞æ‡∞≤‡±Å (3 ‡∞∏‡∞æ‡∞∞‡±ç‡∞≤‡±Å ‡∞™‡±Å‡∞®‡∞∞‡∞æ‡∞µ‡±É‡∞§‡∞Ç ‡∞ö‡±á‡∞Ø‡∞Ç‡∞°‡∞ø):
1. ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞≤‡∞ø‡∞™‡∞ø‡∞≤‡±ã ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞á‡∞µ‡±ç‡∞µ‡∞æ‡∞≤‡∞ø. ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã. ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã.
2. ‡∞Ü‡∞Ç‡∞ó‡±ç‡∞≤ ‡∞Ö‡∞ï‡±ç‡∞∑‡∞∞‡∞æ‡∞≤‡±Å, ‡∞™‡∞¶‡∞æ‡∞≤‡±Å ‡∞µ‡∞æ‡∞°‡∞ï‡±Ç‡∞°‡∞¶‡±Å ‚Äì ‡∞í‡∞ï‡±ç‡∞ï‡∞ü‡∞ø ‡∞ï‡±Ç‡∞°‡∞æ ‡∞ï‡∞æ‡∞¶‡±Å.
3. ‡∞∏‡∞æ‡∞Ç‡∞ï‡±á‡∞§‡∞ø‡∞ï ‡∞™‡∞¶‡∞æ‡∞≤‡∞®‡±Å ‡∞ï‡±Ç‡∞°‡∞æ ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã ‡∞µ‡∞ø‡∞µ‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.
4. ‡∞∏‡±Å‡∞≤‡∞≠‡∞Æ‡±à‡∞® ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å ‡∞™‡∞¶‡∞æ‡∞≤‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.
5. ‡∞µ‡∞ø‡∞¶‡±ç‡∞Ø‡∞æ‡∞∞‡±ç‡∞•‡∞ø‡∞®‡∞ø ‡∞™‡±ç‡∞∞‡±ã‡∞§‡±ç‡∞∏‡∞π‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.
‡∞§‡∞™‡±ç‡∞™‡±Å ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£: "Photosynthesis is..."
‡∞∏‡∞∞‡±à‡∞® ‡∞â‡∞¶‡∞æ‡∞π‡∞∞‡∞£: "‡∞ï‡∞æ‡∞Ç‡∞§‡∞ø ‡∞∏‡∞Ç‡∞∂‡±ç‡∞≤‡±á‡∞∑‡∞£ ‡∞Ö‡∞®‡±á‡∞¶‡∞ø..."
NCERT ‡∞Æ‡∞æ‡∞∞‡±ç‡∞ó‡∞¶‡∞∞‡±ç‡∞∂‡∞ï‡∞æ‡∞≤‡∞®‡±Å ‡∞Ö‡∞®‡±Å‡∞∏‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø."""
}

# ==================== PAGE CONFIG ====================
st.set_page_config(
    page_title="SkillSling - AMD Slingshot 2026",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="collapsed"  # mobile-first: sidebar hidden by default
)

# ==================== PROFESSIONAL DARK UI ====================
st.markdown("""
    <style>
    /* Base theme */
    .stApp {
        background: linear-gradient(135deg, #0b0d11 0%, #1a1d23 100%);
        color: #e3e3e3;
    }
    
    section[data-testid="stSidebar"] {
        background-color: #111216 !important;
        border-right: 2px solid #ed1c24;
    }
    
    /* Chat styling */
    .stChatMessage {
        border-radius: 12px;
        padding: 1.2rem;
        margin-bottom: 0.8rem;
        box-shadow: 0 2px 8px rgba(0,0,0,0.3);
    }
    
    [data-testid="stChatMessageUser"] {
        background: linear-gradient(135deg, #1e2028 0%, #2a2d35 100%) !important;
        border-left: 4px solid #ed1c24;
    }
    
    [data-testid="stChatMessageAssistant"] {
        background: linear-gradient(135deg, #1a1d24 0%, #23252e 100%) !important;
        border-left: 4px solid #00ff88;
    }
    
    /* AMD Branding */
    .amd-badge {
        background: linear-gradient(90deg, #ed1c24 0%, #ff4444 100%);
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 0.75rem;
        font-weight: bold;
        color: white;
        text-transform: uppercase;
        border: 2px solid #ff0000;
        display: inline-block;
        margin: 5px 0;
        box-shadow: 0 0 20px rgba(237, 28, 36, 0.5);
    }
    
    /* Performance metrics */
    .perf-metric {
        font-size: 0.7rem;
        opacity: 0.6;
        font-family: 'Courier New', monospace;
        margin-top: 8px;
        padding: 4px 8px;
        background: rgba(0, 255, 136, 0.1);
        border-radius: 4px;
        display: inline-block;
    }
    
    /* Professional header */
    header[data-testid="stHeader"] {
        background-color: rgba(11, 13, 17, 0.95) !important;
        backdrop-filter: blur(10px);
    }
    
    #MainMenu, footer {visibility: hidden;}
    
    /* Mobile responsive ‚Äì critical for real student phones */
    @media (max-width: 768px) {
        .stChatInput {
            padding-bottom: 80px !important;
            position: sticky !important;
            bottom: 0 !important;
            z-index: 1000 !important;
            background: #0b0d11 !important;
        }
        .stChatMessage {
            font-size: 16px !important;
            padding: 0.8rem !important;
        }
        section[data-testid="stSidebar"] {
            min-width: 0 !important;
            width: 0 !important;
        }
        .stButton button {
            min-height: 44px !important;
            font-size: 16px !important;
        }
        h1 { font-size: 1.8rem !important; }
    }
    
    /* Copy button styling */
    .copy-btn {
        background: #ed1c24;
        color: white;
        border: none;
        padding: 4px 12px;
        border-radius: 4px;
        cursor: pointer;
        font-size: 0.8rem;
        margin-top: 8px;
    }
    
    .copy-btn:hover {
        background: #ff3344;
    }
    </style>
""", unsafe_allow_html=True)

# ==================== SESSION STATE ====================
if "messages" not in st.session_state:
    st.session_state.messages = []
if "subject" not in st.session_state:
    st.session_state.subject = "General"
if "language" not in st.session_state:
    st.session_state.language = "English"
if "model" not in st.session_state:
    st.session_state.model = AVAILABLE_MODELS[0]
if "vector_store" not in st.session_state:
    st.session_state.vector_store = None
if "total_inference_time" not in st.session_state:
    st.session_state.total_inference_time = 0
if "query_count" not in st.session_state:
    st.session_state.query_count = 0
if "last_language" not in st.session_state:
    st.session_state.last_language = "English"
if "language_change_counter" not in st.session_state:
    st.session_state.language_change_counter = 0

# ==================== SIDEBAR ====================
with st.sidebar:
    st.markdown("""
        <div style='text-align: center; margin-bottom: 20px;'>
            <h1 style='color: #ed1c24; margin-bottom: 5px;'>SKILLSLING</h1>
            <div class='amd-badge'>‚ö° AMD SLINGSHOT 2026</div>
            <p style='font-size: 0.75rem; opacity: 0.7; margin-top: 10px;'>
                Offline AI Tutor | 100% Local Inference
            </p>
        </div>
    """, unsafe_allow_html=True)
    
    # Ollama status check
    try:
        ollama.list()
        st.success("‚úÖ Ollama Running")
    except:
        st.error("‚ùå Ollama Not Running!")
        st.caption("Start Ollama: `ollama serve`")
    st.markdown("---")
    
    # Language selection
    st.subheader("üåê Language")
    new_language = st.selectbox(
        "Choose your preferred language",
        ["English", "Hindi", "Hinglish", "Tamil", "Telugu"],
        index=["English", "Hindi", "Hinglish", "Tamil", "Telugu"].index(st.session_state.language),
        help="AI will respond in this language"
    )
    
    # Update language + force placeholder/input refresh
    if new_language != st.session_state.language:
        st.session_state.language = new_language
        st.session_state.last_language = new_language
        st.session_state.language_change_counter += 1
        st.rerun()
    
    # Subject selection
    st.subheader("üìö Subject")
    new_subject = st.selectbox(
        "Select your subject",
        ["General", "Mathematics", "Science", "English", "Social Science"],
        index=["General", "Mathematics", "Science", "English", "Social Science"].index(
            st.session_state.subject if st.session_state.subject != "Maths" else "Mathematics"
        )
    )
    st.session_state.subject = new_subject
    
    # Model selection
    st.subheader("ü§ñ AI Model")
    new_model = st.selectbox(
        "Select AI model",
        AVAILABLE_MODELS,
        index=AVAILABLE_MODELS.index(st.session_state.model),
        help="llama3.2:3b - Best for English\ngemma2:2b - Best for Hindi/Indian languages"
    )
    st.session_state.model = new_model
    st.markdown("---")
    
    # PDF upload
    st.subheader("üìÑ Study Material")
    use_pdf = st.checkbox("Use uploaded PDF context", value=False)
    uploaded_file = st.file_uploader(
        "Upload your notes (PDF)",
        type="pdf",
        help="Max 200MB, first 10 pages will be processed"
    )
    
    if uploaded_file is not None:
        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        with st.spinner("üîÑ Processing PDF..."):
            try:
                loader = PyPDFLoader(tmp_path)
                docs = loader.load()
                splits = RecursiveCharacterTextSplitter(
                    chunk_size=600,
                    chunk_overlap=100
                ).split_documents(docs)
                st.session_state.vector_store = FAISS.from_documents(
                    splits,
                    OllamaEmbeddings(model=st.session_state.model)
                )
                st.success(f"‚úÖ PDF processed ({len(docs)} pages)")
            except Exception as e:
                st.error(f"‚ùå PDF processing failed: {str(e)}")
        os.unlink(tmp_path)
    st.markdown("---")
    
    # Performance stats
    if st.session_state.query_count > 0:
        avg_time = st.session_state.total_inference_time / st.session_state.query_count
        st.subheader("üìä Performance")
        st.metric("Queries Processed", st.session_state.query_count)
        st.metric("Avg Response Time", f"{avg_time:.2f}s")
        st.metric("Total Inference", f"{st.session_state.total_inference_time:.1f}s")
    
    st.markdown("---")
    
    # Actions
    if st.button("üóëÔ∏è Clear Chat", use_container_width=True):
        st.session_state.messages = []
        st.session_state.total_inference_time = 0
        st.session_state.query_count = 0
        st.rerun()
    
    if st.button("üíæ Export Chat (JSON)", use_container_width=True):
        if st.session_state.messages:
            chat_data = {
                "export_time": datetime.now().isoformat(),
                "language": st.session_state.language,
                "model": st.session_state.model,
                "subject": st.session_state.subject,
                "messages": st.session_state.messages
            }
            st.download_button(
                "Download JSON",
                json.dumps(chat_data, ensure_ascii=False, indent=2),
                file_name=f"skillsling_chat_{int(time.time())}.json",
                mime="application/json"
            )
    st.markdown("---")
    st.caption("Built by SkillSling Team")
    st.caption("Powered by AMD GPUs")

# ==================== MAIN CONTENT ====================
if not st.session_state.messages:
    st.markdown("""
        <div style='text-align:center; padding: 60px 20px;'>
            <h1 style='color:#ed1c24; font-size: 3rem; margin-bottom: 10px;'>
                üöÄ SkillSling AI
            </h1>
            <p style='font-size: 1.2rem; opacity: 0.8; margin-bottom: 30px;'>
                Your Offline Intelligent Tutor
            </p>
            <div class='amd-badge' style='font-size: 0.9rem;'>
                ‚ö° POWERED BY AMD SLINGSHOT
            </div>
            <p style='margin-top: 30px; font-size: 0.95rem; opacity: 0.7;'>
                ‚Ä¢ Multi-language support (5 languages)<br>
                ‚Ä¢ 100% offline & private<br>
                ‚Ä¢ PDF note integration<br>
                ‚Ä¢ NCERT-aligned responses
            </p>
        </div>
    """, unsafe_allow_html=True)

# Display chat history with performance metrics
for idx, msg in enumerate(st.session_state.messages):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])
        
        # Show performance metrics for assistant messages
        if msg["role"] == "assistant" and "latency" in msg:
            st.markdown(
                f"<div class='perf-metric'>‚ö° {msg['latency']:.2f}s | AMD Optimized</div>",
                unsafe_allow_html=True
            )

# Dynamic placeholder + force re-mount on language change
current_placeholder = PLACEHOLDERS.get(st.session_state.language, "Type your question...")
prompt = st.chat_input(
    current_placeholder,
    key=f"chat_input_{st.session_state.language}_{st.session_state.get('language_change_counter', 0)}"
)

# Force rerun when language changes to refresh placeholder
if st.session_state.language != st.session_state.get("last_language", "English"):
    st.session_state.language_change_counter = st.session_state.get("language_change_counter", 0) + 1
    st.session_state.last_language = st.session_state.language
    st.rerun()

# ==================== CHAT LOGIC ====================
if prompt:
    # Debug print to terminal ‚Äì you can remove later
    print(f"DEBUG: INPUT CAPTURED! Prompt: '{prompt}' | Language: {st.session_state.language} | Model: {st.session_state.model}")

    # Add user message
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    with st.chat_message("user"):
        st.markdown(prompt)
    
    with st.chat_message("assistant"):
        placeholder = st.empty()
        full_response = ""
        
        # BUILD MESSAGES WITH ULTRA-STRONG LANGUAGE ENFORCEMENT
        ollama_messages = []
        
        # 1. Add language-specific system prompt FIRST (most important)
        language_prompt = LANGUAGE_SYSTEM_PROMPTS.get(
            st.session_state.language,
            LANGUAGE_SYSTEM_PROMPTS["English"]
        )
        ollama_messages.append({
            "role": "system",
            "content": f"{language_prompt}\n\nSubject Focus: {st.session_state.subject}\nCurrent Date: {datetime.now().strftime('%Y-%m-%d')}"
        })
        
        # 2. Add first-message language reinforcement (forces model to obey from reply #1)
        if len(st.session_state.messages) == 2:  # right after first user message
            ollama_messages.append({
                "role": "user",
                "content": f"Important reminder: From now on, reply ONLY in {st.session_state.language}. No English at all. Start now."
            })
        
        # 3. Add PDF context if available
        if use_pdf and st.session_state.vector_store:
            try:
                docs = st.session_state.vector_store.similarity_search(prompt, k=3)
                context = "\n\n".join([d.page_content for d in docs])
                ollama_messages.append({
                    "role": "system",
                    "content": f"PDF Context (use this for answering):\n{context[:2000]}"
                })
            except:
                pass
        
        # 4. Add conversation history
        for msg in st.session_state.messages:
            ollama_messages.append(msg)
        
        # 5. Generate response with timing
        start_time = time.time()
        with st.spinner(f"ü§î Thinking in {st.session_state.language}..."):
            try:
                stream = ollama.chat(
                    model=st.session_state.model,
                    messages=ollama_messages,
                    stream=True,
                    options={
                        "temperature": 0.3,
                        "num_predict": 1024,
                        "top_p": 0.9
                    }
                )
                
                for chunk in stream:
                    if 'message' in chunk and 'content' in chunk['message']:
                        full_response += chunk['message']['content']
                        placeholder.markdown(full_response + "‚ñå")
                        time.sleep(0.01)
                
                latency = time.time() - start_time
                placeholder.markdown(full_response)
                
                # Update performance stats
                st.session_state.total_inference_time += latency
                st.session_state.query_count += 1
                
                # Show performance metric
                st.markdown(
                    f"<div class='perf-metric'>‚ö° {latency:.2f}s | AMD Optimized | {len(full_response)} chars</div>",
                    unsafe_allow_html=True
                )
                
                # Save assistant message with metadata
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": full_response,
                    "latency": latency,
                    "timestamp": datetime.now().isoformat()
                })
                
            except Exception as e:
                error_msg = f"‚ùå **Error:** {str(e)}\n\n**Troubleshooting:**\n1. Check if Ollama is running: `ollama serve`\n2. Verify model is loaded: `ollama list`\n3. Try pulling model: `ollama pull {st.session_state.model}`"
                placeholder.markdown(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })

# ==================== FOOTER ====================
st.markdown("---")
col1, col2, col3 = st.columns(3)
with col1:
    st.caption(f"üåê Language: **{st.session_state.language}**")
with col2:
    st.caption(f"ü§ñ Model: **{st.session_state.model}**")
with col3:
    st.caption(f"üìö Subject: **{st.session_state.subject}**")
st.markdown("""
    <div style='text-align: center; margin-top: 20px; padding: 20px; opacity: 0.6;'>
        <p style='font-size: 0.85rem;'>
            üí° <b>Pro Tip:</b> Change language anytime ‚Äì your chat history stays preserved!
        </p>
        <p style='font-size: 0.75rem; margin-top: 10px;'>
            SkillSling AI | AMD Slingshot 2026 | 100% Offline & Private
        </p>
    </div>
""", unsafe_allow_html=True)